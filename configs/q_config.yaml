# W&B Configuration
use_wandb: true
wandb_project: "demand_forecasting_quantum_comparison" 
wandb_entity: null

# Data Configuration
# data_path, checkpoint_path, and saved_model_path are provided via command-line arguments at runtime.
forecasting_type: "univariate"
target_column: "Demand(MW)"
features: [] 

# --- NEW: Feature Engineering Control Panel ---
feature_engineering:
  use_time_features: false              # (true/false) Create hour, day, month features?
  use_cyclical_encoding: false          # (true/false) Convert time features to sin/cos?
  use_lag_features: false               # (true/false) Create demand_lag_24hr, etc.?
  use_rolling_window_features: false    # (true/false) Create rolling mean/std?


# Model Configuration
model_type: "QEnhancedLSTM" 

# --- Parameters for QEnhancedLSTMModel ---
classical_lstm_hidden_size: 16
num_classical_lstm_layers: 2 
dropout: 0.1     
hidden_size: 7

# Quantum Enhancement part
n_qubits_qelstm: 7
n_pqc_layers_qelstm: 2
ansatz_type_qelstm: "StronglyEntanglingLayers"

# FC layers after concatenation in QEnhancedLSTM
qelstm_fc_hidden1_dim: 4
qelstm_fc_hidden2_dim: 0
qelstm_fc_dropout: 0.1

# --- General Training Parameters ---
input_size: 1 
sequence_length: 24 
forecast_horizon: 1
batch_size: 64 
learning_rate: 0.01
num_epochs: 20
patience: 10 
random_state: 42 
test_size: 0.2 
val_size: 0.2 

# Checkpointing
checkpoint_freq: 5

scheduler_config:
  use_scheduler: True   # Set to True to enable the scheduler
  patience: 3           # Reduce LR if val_loss doesn't improve for 3 epochs
  factor: 0.5           # When reducing, new_lr = old_lr * 0.5
